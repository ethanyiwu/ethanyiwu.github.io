---
---

@string{aps = {American Physical Society,}}

@misc{chuang2023evolving,
      abbr={arXiv},
      bibtex_show={true},
      title={Evolving Domain Adaptation of Pretrained Language Models for Text Classification},
      author={Yun-Shiuan Chuang and Yi Wu and Dhruv Gupta and Rheeya Uppaal and Ananya Kumar and Luhang Sun and Makesh Narsimhan Sreedhar and Sijia Yang and Timothy T. Rogers and Junjie Hu},
      year={2023},
      eprint={2311.09661},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      selected={true},
      url={https://arxiv.org/abs/2311.09661},
      html={https://arxiv.org/abs/2311.09661}
}

@inproceedings{
chuang2023evolving,
title={Evolving Domain Adaptation of Pretrained Language Models for Text Classification},
author={Yun-Shiuan Chuang and Rheeya Uppaal and Yi Wu and Luhang Sun and Makesh Narsimhan Sreedhar and Sijia Yang and Timothy T. Rogers and Junjie Hu},
booktitle={NeurIPS 2023 Workshop on Distribution Shifts: New Frontiers with Foundation Models},
year={2023},
url={https://openreview.net/forum?id=HSBV4cCheG}
}

@inproceedings{wu-etal-2023-knowcomp,
    title = "{K}now{C}omp Submission for {WMT}23 Word-Level {A}uto{C}ompletion Task",
    author = "Wu, Yi  and
      Shi, Haochen  and
      Wang, Weiqi  and
      Song, Yangqiu",
    editor = "Koehn, Philipp  and
      Haddow, Barry  and
      Kocmi, Tom  and
      Monz, Christof",
    booktitle = "Proceedings of the Eighth Conference on Machine Translation",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.wmt-1.79",
    doi = "10.18653/v1/2023.wmt-1.79",
    pages = "882--889",
    abstract = "The NLP community has recently witnessed the success of Large Language Models (LLMs) across various Natural Language Processing (NLP) tasks. However, the potential of LLMs for word-level auto-completion in a multilingual context has not been thoroughly explored yet. To address this gap and benchmark the performance of LLMs, we propose an LLM-based system for the WMT23 Word-Level Auto-Completion (WLAC) task. Our system utilizes ChatGPT to represent LLMs and evaluates its performance in three translation directions: Chinese-English, German-English, and English-German. We also study the task under zero-shot and few-shot settings to assess the potential benefits of incorporating exemplars from the training set in guiding the LLM to perform the task. The results of our experiments show that, on average, our system attains a 29.8{\%} accuracy on the test set. Further analyses reveal that LLMs struggle with WLAC in the zero-shot setting, but performance significantly improves with the help of additional exemplars, though some common errors still appear frequently. These findings have important implications for incorporating LLMs into computer-aided translation systems, as they can potentially enhance the quality of translations. Our codes for evaluation are available at https://github.com/ethanyiwu/WLAC.",
}
